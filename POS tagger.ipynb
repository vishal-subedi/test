{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "500bc359-43c1-45ae-a051-1ec51a68d1b0"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.957925440524\n",
      "A prince named Abdullah whose wife was Chuchi had big boobs.\n",
      "['A', 'prince', 'named', 'Abdullah', 'whose', 'wife', 'was', 'Chuchi', 'had', 'big', 'boobs', '.']\n",
      "['DT', 'NN', 'VBD', 'NNP', 'WP$', 'NN', 'VBD', 'NNP', 'VBD', 'JJ', 'NNS', '.']\n",
      " Abdullah\n",
      " Chuchi\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.corpus import state_union, treebank\n",
    "tagged_sentences=treebank.tagged_sents()\n",
    "#print(tagged_sentences[:5])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "#defining features\n",
    "\n",
    "def features(sentence, index):\n",
    "    #\"\"\" sentence: array of words, index: the index of the word \"\"\"\n",
    "    return {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "#function to strip  words in the form of an array from the given sentence \n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]\n",
    "\n",
    "training_sentences, test_sentences=train_test_split(tagged_sentences,random_state=7)\n",
    "#print(training_sentences)\n",
    "\n",
    "#function to transform into feature and response matrix.\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y \n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "#X contains features of all the words in the sentence argument passed in the form of a dictionary.\n",
    "#y contains all the tags of the sentences in the form of dictionary.\n",
    "\n",
    "#training the classifier\n",
    "\n",
    "clf=Pipeline([('vectorizer', DictVectorizer(sparse='False')),\n",
    "              ('classifier', svm.SVC(kernel='rbf', C=7000, random_state=7, decision_function_shape='ovr'))])\n",
    "clf.fit(X_train, y_train)\n",
    "#print(clf.feature_names_)\n",
    "X_test, y_test=transform_to_dataset(test_sentences)\n",
    "print(clf.score(X_test,y_test))\n",
    "\n",
    "#predicting the values\n",
    "\n",
    "def tag(sentence):\n",
    "    tags=clf.predict([features(sentence,index) for index in range(len(sentence))])\n",
    "    return list(zip(sentence,tags))\n",
    "\n",
    "#using the POS tagger to predict\n",
    "\n",
    "text=input()\n",
    "token=word_tokenize(text)\n",
    "tagged=tag(token)\n",
    "ner=[]\n",
    "pos=[]\n",
    "for i,j in tagged:\n",
    "    ner.append(i)\n",
    "    pos.append(j)\n",
    "print(ner)\n",
    "print(pos)\n",
    "A=ner\n",
    "B=pos\n",
    "B.append(\"!!!\")\n",
    "ans = []\n",
    "temp = \"\"\n",
    "Len = len(B)\n",
    "for i in range(0, Len - 1):\n",
    "\tif(B[i] == \"NNP\" and B[i + 1] == \"NNP\"):\n",
    "\t\ttemp += A[i]\n",
    "\telse:\n",
    "\t\tif(B[i] == \"NNP\"):\n",
    "\t\t\ttemp += \" \" + A[i]\n",
    "\t\tTempLen = len(temp)\n",
    "\t\tif(TempLen):\n",
    "\t\t\tans.append(temp)\n",
    "\t\ttemp = \"\"\n",
    "\n",
    "\n",
    "Len = len(ans)\n",
    "for i in range(0, Len):\n",
    "    print(ans[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
